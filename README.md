# ğŸ¿ Netflix Movie Recommender System

**A production-ready template for a real-time movie recommendation platform** combining

* Collaborative filtering (LightFM/ALS)
* Feature Store (Feast)
* Data versioning (DVC + Azure Blob Storage)
* Experiment tracking (MLflow)
* Inference API (FastAPI)
* Streaming & analytics (Kafka â†’ Flink â†’ Data Lake)
* Auto-retraining & drift detection (Evidently, ZenML/Airflow)
* CI/CD pipeline (GitHub Actions)

---

## ğŸ“‹ Features

* **User & Item Features**: offline and online feature store using Feast
* **Recommendation Model**: LightFM (hybrid) or ALS (matrix factorization)
* **Experiment Tracking**: MLflow logs hyperparameters, metrics, and model artifacts
* **Data Versioning**: DVC pipelines manage raw and processed data with remote storage on Azure Blob
* **Real-time Events**: Click/view events streamed via Kafka, processed by Flink, stored in Data Lake
* **Inference Service**: FastAPI endpoint serving recommendations using real-time features
* **Drift Detection**: Evidently reports identify feature and concept drift
* **Auto-Retraining**: Scheduled retraining pipelines (ZenML or GitHub Actions cron)
* **CI/CD**: Automated build, test, docker image push, and deploy via GitHub Actions

---

## ğŸ—ï¸ Architecture Overview

```text
[User] â†’ FastAPI (click logging â†’ Kafka)
              â†“
         Kafka Topic (user_events)
              â†“
        Apache Flink Streaming
              â†“
        Data Lake (Parquet partitions)
              â†“
      DVC & Feast Offline Store
              â†“              â†˜
        Training Pipeline â†’ MLflow â†’ Model Registry
              â†“                                   â†˜
  Retraining Cron / Trigger                      Inference API â†’ FastAPI â†’ Users
              â†‘
      Feast Online Store (Redis)
```

---

## âš™ï¸ Prerequisites

* **OS**: Linux (tested)
* **Docker**
* **Python 3.10+**
* **Azure CLI** (if using Azure Blob remote)
* **kubectl** & **Helm** (optional, for K8s deploy)
* **Kafka**, **Redis**, **Flink** (locally via Docker)

---

## ğŸš€ Quickstart

1. **Clone the repository**:

   ```bash
   git clone https://github.com/yourname/netflix-recommender-template.git
   cd netflix-recommender-template
   ```

2. **Install Python dependencies**:

   ```bash
   pip install -r requirements.txt
   ```

3. **Configure environment variables**:

   * Copy `.env.example` to `.env` and fill in credentials:

     ```bash
     cp .env.example .env
     ```
   * Required vars: `AZURE_ACCOUNT`, `AZURE_KEY`, `REDIS_URL`, `MLFLOW_TRACKING_URI`, etc.

4. **Initialize DVC & pull data**:

   ```bash
   dvc pull
   ```

5. **Apply Feast feature store**:

   ```bash
   feast apply
   feast materialize-incremental $(date +%F)
   ```

6. **Run data pipeline** (
   ingest â†’ preprocess â†’ feature) with DVC:

   ```bash
   dvc repro
   ```

7. **Train the recommendation model**:

   ```bash
   python src/train.py
   ```

8. **Launch MLflow UI**:

   ```bash
   mlflow ui --port 5000
   ```

9. **Start the inference API**:

   ```bash
   uvicorn src.api:app --host 0.0.0.0 --port 8000 --reload
   ```

10. **View documentation & test**:

    * Visit `http://localhost:8000/docs` for Swagger UI
    * Test recommendation endpoint:

      ```bash
      curl 'http://localhost:8000/recommend?user_id=1&movie_id=50'
      ```

---

## ğŸ—ƒï¸ Project Structure

```
netflix-recommender-template/
â”œâ”€â”€ data/                    # DVC-tracked raw & processed data
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”‚
â”œâ”€â”€ src/                     # Application code
â”‚   â”œâ”€â”€ ingest.py            # Data ingestion
â”‚   â”œâ”€â”€ preprocess.py        # Data cleaning
â”‚   â”œâ”€â”€ features/            # Feast definitions & config
â”‚   â”‚   â”œâ”€â”€ feature_store.yaml
â”‚   â”‚   â””â”€â”€ feature_def.py
â”‚   â”œâ”€â”€ train.py             # Model training & MLflow
â”‚   â”œâ”€â”€ evaluate_drift.py    # Drift detection (Evidently)
â”‚   â”œâ”€â”€ api.py               # FastAPI inference
â”‚   â””â”€â”€ simulator/           # RecSim + Kafka producer
â”‚       â””â”€â”€ recsim_kafka.py
â”‚
â”œâ”€â”€ model/                   # Local model artifacts or MLflow URIs
â”œâ”€â”€ tests/                   # Unit tests (pytest)
â”‚
â”œâ”€â”€ .github/                 # GitHub Actions workflows
â”‚   â””â”€â”€ deploy.yml
â”‚
â”œâ”€â”€ Dockerfile               # Containerize FastAPI + simulator
â”œâ”€â”€ dvc.yaml                 # DVC pipeline config
â”œâ”€â”€ dvc.lock
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example             # Environment variables template
â””â”€â”€ README.md                # This file
```

---

## ğŸ”„ CI/CD Pipeline

* **GitHub Actions** defined in `.github/workflows/deploy.yml`:

  * Lint & test (pytest)
  * Build & push Docker image
  * Trigger deployment hook (e.g. Render/Fly.io/Azure)

---

## â˜ï¸ Deployment Options

* **Docker Compose** (local testing)
* **Render/Fly.io**: automatic Docker deploy
* **Azure App Service**: containerized deployment
* **Kubernetes**: Helm chart for FastAPI, Kafka, Redis, Flink

---

## ğŸ›ï¸ Real-time Simulation & Streaming

1. **RecSim + Kafka**: `src/simulator/recsim_kafka.py` sends simulated user events
2. **Flink Job**: defined in `infra/flink/job.sql` or PyFlink script
3. **Data Lake**: Parquet files partitioned by event type/day under `/data_lake/`

---

## ğŸ¤ Contributing

1. Fork this repo
2. Create a branch: `feature/your-feature`
3. Commit changes & push
4. Open a Pull Request

---

## ğŸ“„ License

MIT License Â© 2025 Minh TranBinh
